# Major Issues/Changes Addressed in SQL Agent Chat Session

## 1. ASYNC TIMEOUT ISSUE (execute_db.py)
**Problem:** 
- SQL queries were timing out with CancelledError
- Agent would make tool calls but get null output
- BigQuery queries executed successfully but results never returned to agent
- Issue was in async wrapper using run_in_executor() causing thread executor timeouts

**Solution:** 
- Refactored execute_query() to be directly async instead of using sync + wrapper
- Removed arun() wrapper function that used run_in_executor()
- Made execute_query() handle BigQuery operations asynchronously with proper await calls
- Updated tool definition to use execute_query as both func and coroutine

**Files Changed:** agents/tools/execute_db.py

## 2. SQL AGENT PROMPT OPTIMIZATION (main2.py)
**Problem:**
- Overly verbose and confusing prompt
- Complex sample queries in schema docs were misleading the agent
- No clear error handling guidelines or retry limits
- Missing BigQuery-specific syntax guidance
- Unclear success/failure criteria leading to infinite retry loops

**Solution:**
- Simplified opening from verbose multi-paragraph to single clear sentence
- Changed from "Generate and execute BigQuery SQL" to more organic "Answer user questions using tools available"
- Added "BEFORE WRITING SQL" planning step to catch logic errors early
- Set clear retry limits (MAX 3 attempts)
- Added BigQuery-specific syntax notes and common gotchas
- Removed confusing sample queries from schema display
- Streamlined response format with clear success/error criteria

**Files Changed:** agents/sql/main2.py

## 3. AGENT CONTAMINATION ISSUE (Multi-agent behavior)
**Problem:**
- Search agent was mimicking SQL agent's response format
- Agents seeing shared conversation history and copying patterns from other agents
- Search agent saying "Results successfully gathered" when it only did searches
- Loss of distinct agent identities due to pattern reinforcement in shared context

**Solution Identified (Not Yet Implemented):**
- Add distinct response format constraints for each agent type
- Search agent: Must start with "Search complete:" or "Found:"
- SQL agent: Must start with "SQL Results obtained:"
- Visualiser: Must start with "Visualization created:"
- Add agent identity enforcement in supervisor prompt
- Create strong behavioral anchors that resist contamination

**Files to Change:** 
- agents/search/main.py
- agents/sql/main2.py  
- agents/visualiser/main2.py
- agents/sql_with_preprocess/supervisor.py

## 4. HEROKU R14 MEMORY QUOTA EXCEEDED (Memory Optimization)
**Problem:**
- Heroku logs showing R14 errors: 565M memory usage exceeding ~512MB limit
- Application consuming excessive memory leading to crashes
- Multiple memory leaks identified:
  - Graph recompilation on every request (50-100MB each)
  - Vector store caching keeping all databases in memory (200-400MB)
  - Unbounded message accumulation in agent conversations

**Root Causes:**
- `create_graph()` called for every chat request instead of caching
- `VectorStoreManager` permanently cached all vector stores in memory
- Agent state messages growing without bounds during conversations

**Solution - Graph Caching:**
- Implemented global graph cache with thread-safe singleton pattern
- Added `get_cached_graph()` function with asyncio.Lock() for concurrency
- Graph compiled once on first request, reused for all subsequent requests
- LangGraph is stateless/thread-safe, so multiple users can safely share same instance

**Solution - Vector Store On-Demand Querying:**
- Removed all vector store caching (`self.stores = {}` and `self.retrievers = {}`)
- Implemented query-on-demand pattern for Qdrant cloud storage
- Create `QdrantVectorStore` connection per query, let it be garbage collected
- Only keep lightweight Qdrant client (~5MB) instead of full cached stores
- For local FAISS: minimal caching only when needed

**Memory Impact:**
- Graph caching: 50-100MB saved per request after first
- Vector store optimization: 295MB permanent reduction
- Total memory savings: ~345-395MB
- Should eliminate R14 errors and bring usage well under 512MB limit

**Files Changed:** 
- backend/app/main.py (graph caching)
- backend/app/api/chat.py (use cached graph)
- agents/utils/vector_stores.py (on-demand querying)

## 5. HEROKU H12/H15 TIMEOUT ERRORS (Stream Response Fix)
**Problem:**
- H12 (Request timeout): 30-second limit exceeded during long agent processing
- H15 (Idle connection): 55-second limit exceeded due to gaps in streaming output
- Users experiencing timeouts when agents took time to process queries
- Multi-agent workflows causing extended periods of silence before final response

**Root Cause:**
- `app.astream()` without `subgraphs=True` was only streaming final supervisor output
- Intermediate agent processing (search → SQL → visualizer) created long silent periods
- Heroku terminated connections during these processing gaps

**Solution:**
- Added `subgraphs=True` parameter to `app.astream()` call in streaming response
- Enables real-time streaming of intermediate agent outputs
- Each agent's processing now streams immediately instead of buffering
- Keeps connection alive throughout entire multi-agent workflow

**Technical Details:**
```python
# Before: Silent processing, then timeout
async for chunk in app.astream(initial_state, stream_mode=["updates"]):

# After: Continuous streaming prevents timeout  
async for chunk in app.astream(initial_state, 
                               subgraphs=True,
                               stream_mode=["updates"]):
```

**Files Changed:** 
- backend/app/api/chat.py (streaming response fix)

## 6. VECTOR STORE SINGLETON PATTERN (Memory Leak Fix)
**Problem:**
- VectorStoreManager was being instantiated at module import time
- Every request to search agent created a NEW VectorStoreManager instance
- Each instance loaded OpenAI embeddings model (~50-100MB) and Qdrant client
- On Heroku, this caused multiple instances per process leading to memory bloat

**Root Cause:**
```python
# In agents/tools/search_vectordb.py - BAD
vector_store = VectorStoreManager()  # Created on every import
```

**Solution:**
- Implemented lazy singleton pattern for VectorStoreManager
- Changed from immediate instantiation to on-demand creation
- Global `_vector_store` variable with `get_vector_store()` function
- Ensures exactly ONE VectorStoreManager instance per process lifetime
- Heavy resources (embeddings, Qdrant client) only loaded once and reused

**Technical Implementation:**
```python
# New singleton pattern
_vector_store = None

def get_vector_store():
    global _vector_store
    if _vector_store is None:
        _vector_store = VectorStoreManager()
    return _vector_store
```

**Memory Impact:**
- Before: 2-3+ VectorStoreManager instances = 150-300MB per process
- After: Exactly 1 VectorStoreManager instance = 50-100MB total
- Memory reduction: ~100-200MB per process
- Combined with previous optimizations, should eliminate R14 memory errors

**Files Changed:**
- agents/tools/search_vectordb.py (singleton pattern implementation)

## Summary
These changes address: (1) Technical async execution issues, (2) Prompt engineering for better SQL generation, (3) Multi-agent system behavioral consistency to prevent role confusion, (4) Critical memory optimization to resolve Heroku deployment issues, and (5) Vector store memory leak elimination to ensure stable operation within memory limits. 